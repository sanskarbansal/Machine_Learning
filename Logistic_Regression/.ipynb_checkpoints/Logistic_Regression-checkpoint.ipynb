{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis Function**\n",
    "___\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " x^1_1 & x^1_2 & .  & . & x^1_n \\\\ \n",
    " x^2_1 & x^2_2 & .  & . & x^2_n \\\\\n",
    " x^3_1 & x^3_2 & .  & . & x^3_n \\\\\n",
    " . & . & . &  . & . \\\\\n",
    " . & . & . &  . & . \\\\\n",
    " x^n_1 & . & . & . & x^n_n \\\\\n",
    "\\end{bmatrix} \\cdot \n",
    "\\begin{bmatrix}\n",
    "m_1\\\\ \n",
    "m_2\\\\ \n",
    "m_3\\\\ \n",
    ". \\\\ \n",
    ". \\\\ \n",
    "m_n \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    " g(m_1 \\times x^1_1 + m_2 \\times x^1_2 + ..... + m_n \\times x^1_n) \\\\ \n",
    " g(m_1 \\times x^2_1 + m_2 \\times x^2_2 + ..... + m_n \\times x^2_n) \\\\ \n",
    " g(m_1 \\times x^3_1 + m_2 \\times x^3_2 + ..... + m_n \\times x^3_n) \\\\ \n",
    " .  \\\\\n",
    " . \\\\\n",
    " g(m_1 \\times x^n_1 + m_2 \\times x^n_2 + ..... + m_n \\times x^n_n) \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br/>\n",
    "$$\n",
    "g(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Basic Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X): \n",
    "    return 1.0/(1.0+np.exp(-X))\n",
    "def hypothesis(X, m): \n",
    "    return sigmoid(np.dot(X, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Function**\n",
    "___\n",
    "\n",
    "$$\n",
    "Loss = \\frac{\\sum_{1}^{n}(y^ilog(h^i) + (1-y^i)log(1-h^i))}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(X, Y, m): \n",
    "    h = hypothesis(X, m)\n",
    "    return -1*np.mean((Y*np.log(h)) + ((1-Y)*(np.log(1-h))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "___\n",
    "$$\n",
    "m = m - lr*\\frac{\\partial{loss}}{\\partial{m}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{loss}}{\\partial{m_j}} = -\\sum^{n}_{i = 1}\\frac{y^i-g(m^Tx^i)}{n} \\cdot x^i_j\n",
    "$$\n",
    "\n",
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, m): \n",
    "    mtx = hypothesis(X, m)\n",
    "    n = X.shape[0]\n",
    "    return  np.dot(X.T, (Y - mtx))*(-1/n)\n",
    "def gradient_descent(X, Y, lr = 0.1, max_iterations = 100, verbose = True): \n",
    "    m = np.zeros((X.shape[1], 1))\n",
    "    for _ in range(max_iterations): \n",
    "        if verbose: \n",
    "            print(f\"Error: {error(X, Y, m)}\")\n",
    "        m = m - lr*gradient(X, Y, m)\n",
    "    return m; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, m): \n",
    "    h = hypothesis(X, m)\n",
    "    output = np.zeros(h.shape)\n",
    "    output[h >= 0.5] = 1\n",
    "    output[h < 0.5] = 0\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
